import { ProxyConfig } from "./config.ts";
import { ClaudeRequest } from "./types.ts";
import { SSEWriter } from "./sse.ts";
import { log, logPhase, LogPhase } from "./logging.ts";
import { enrichClaudeRequest } from "./prompt_inject.ts";
import { mapClaudeToOpenAI } from "./map_claude_to_openai.ts";
import { handleOpenAIStream } from "./handle_openai_stream.ts";
import { handleAnthropicStream } from "./handle_anthropic_stream.ts";
import { countTokensWithTiktoken } from "./tiktoken.ts";
import { ToolifyParser } from "./parser.ts";
import { countTokensLocally } from "./token_counter.ts";
import { ToolCallDelimiter } from "./signals.ts";

export async function forwardRequest(
  request: ClaudeRequest,
  writer: SSEWriter | undefined,
  config: ProxyConfig,
  requestId: string,
  clientApiKey?: string,
  abortSignal?: AbortSignal,
) {
  // 1. è‡ªåŠ¨é€‰æ‹©ä¸Šæ¸¸é…ç½®
  let baseUrl: string;
  let apiKey: string | undefined;
  let requestModel: string;
  let protocol: "openai" | "anthropic";

  // è§£ææ¨¡å‹åï¼šæ”¯æŒ "channel+model" æ ¼å¼
  const modelName = request.model;
  const plusIndex = modelName.indexOf("+");
  
  if (plusIndex !== -1) {
    const channelName = modelName.slice(0, plusIndex);
    const actualModel = modelName.slice(plusIndex + 1);
    const channel = config.channelConfigs.find(c => c.name === channelName);
    
    if (channel) {
      baseUrl = channel.baseUrl;
      apiKey = channel.apiKey;
      requestModel = actualModel;
      protocol = channel.protocol ?? config.defaultProtocol;
    } else {
      // æ‰¾ä¸åˆ°æ¸ é“ï¼Œå›é€€é€»è¾‘
      baseUrl = config.upstreamBaseUrl!;
      apiKey = config.upstreamApiKey;
      requestModel = modelName;
      protocol = config.defaultProtocol;
    }
  } else {
    // æ²¡å¸¦å†’å·ï¼Œä½¿ç”¨é»˜è®¤ä¸Šæ¸¸æˆ–ç¬¬ä¸€ä¸ªæ¸ é“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if (config.channelConfigs.length > 0) {
      const channel = config.channelConfigs[0];
      baseUrl = channel.baseUrl;
      apiKey = channel.apiKey;
      requestModel = modelName;
      protocol = channel.protocol ?? config.defaultProtocol;
    } else {
      baseUrl = config.upstreamBaseUrl!;
      apiKey = config.upstreamApiKey;
      requestModel = config.upstreamModelOverride ?? modelName;
      protocol = config.defaultProtocol;
    }
  }

  // å¦‚æœå¯ç”¨äº†é€ä¼  API keyï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®¢æˆ·ç«¯æä¾›çš„ key
  if (config.passthroughApiKey && clientApiKey) {
    apiKey = clientApiKey;
  }

  // 2. å¢å¼ºè¯·æ±‚ï¼ˆæ³¨å…¥å·¥å…·ã€å¤„ç† Tool Blocksï¼‰
  const { request: enrichedRequest, delimiter } = enrichClaudeRequest(request);
  
  if (delimiter && request.tools && request.tools.length > 0) {
    logPhase(requestId, LogPhase.ENRICHED, `Injected ${request.tools.length} tools`, {
      delimiter: delimiter.getMarkers().TC_START,
    });
  }

  // 3. å‡†å¤‡è½¬å‘è¯·æ±‚
  let fetchBody: string;
  let finalUrl = baseUrl;
  const headers: Record<string, string> = {
    "Content-Type": "application/json",
  };

  const isStream = request.stream === true;

  if (protocol === "openai") {
    const openaiReq = mapClaudeToOpenAI(enrichedRequest, requestModel);
    openaiReq.stream = isStream;
    fetchBody = JSON.stringify(openaiReq);
    if (apiKey) {
      headers["Authorization"] = `Bearer ${apiKey}`;
    }
  } else {
    // Anthropic åè®®
    const anthropicReq = {
      ...enrichedRequest,
      model: requestModel,
      stream: isStream,
    };
    fetchBody = JSON.stringify(anthropicReq);
    if (apiKey) {
      headers["x-api-key"] = apiKey;
    }
    headers["anthropic-version"] = "2023-06-01";
  }

  logPhase(requestId, LogPhase.UPSTREAM, `Forwarding to ${protocol.toUpperCase()}`, {
    model: requestModel,
    url: finalUrl.split("/").pop(),
  });

  // 4. å‘é€è¯·æ±‚
  const upstreamStartTime = Date.now();
  const response = await fetch(finalUrl, {
    method: "POST",
    headers,
    body: fetchBody,
    signal: abortSignal,
  });

  if (!response.ok) {
    const errorText = await response.text();
    logPhase(requestId, LogPhase.ERROR, `Upstream failed (${response.status})`, {
      error: errorText.slice(0, 200),
    });
    throw new Error(`Upstream returned ${response.status}: ${errorText}`);
  }
  
  const ttfb = Date.now() - upstreamStartTime;
  logPhase(requestId, LogPhase.STREAM, `Receiving response (TTFB: ${ttfb}ms)`);

  // 5. å¤„ç†å“åº”
  const thinkingEnabled = request.thinking?.type === "enabled";
  
  // ä½¿ç”¨å¢å¼ºåçš„æœ¬åœ°è®¡æ•°å™¨è®¡ç®—è¾“å…¥ Token
  const localUsage = await countTokensLocally(enrichedRequest, config, requestId);
  const inputTokens = localUsage.input_tokens;

  if (isStream && writer) {
    if (protocol === "openai") {
      const result = await handleOpenAIStream(
        response,
        writer,
        config,
        requestId,
        delimiter,
        thinkingEnabled,
        inputTokens,
        request.model, // ä¼ å…¥åŸå§‹æ¨¡å‹å
        enrichedRequest.messages, // ğŸ”‘ ä¼ é€’åŸå§‹æ¶ˆæ¯ç”¨äºé‡è¯•
        finalUrl, // ğŸ”‘ ä¼ é€’ä¸Šæ¸¸ URL
        headers, // ğŸ”‘ ä¼ é€’è¯·æ±‚å¤´
        protocol, // ğŸ”‘ ä¼ é€’åè®®ç±»å‹
      );
      return { inputTokens, outputTokens: result?.outputTokens };
    } else {
      const result = await handleAnthropicStream(
        response,
        writer,
        config,
        requestId,
        delimiter,
        thinkingEnabled,
        inputTokens,
        request.model, // ä¼ å…¥åŸå§‹æ¨¡å‹å
        enrichedRequest.messages, // ğŸ”‘ ä¼ é€’åŸå§‹æ¶ˆæ¯ç”¨äºé‡è¯•
        finalUrl, // ğŸ”‘ ä¼ é€’ä¸Šæ¸¸ URL
        headers, // ğŸ”‘ ä¼ é€’è¯·æ±‚å¤´
        protocol, // ğŸ”‘ ä¼ é€’åè®®ç±»å‹
      );
      return { inputTokens, outputTokens: result?.outputTokens };
    }
  } else {
    // éæµå¼å“åº”å¤„ç†
    const json = await response.json();
    if (protocol === "openai") {
      // å°† OpenAI éæµå¼å“åº”è½¬æ¢ä¸º Claude æ ¼å¼
      const message = json.choices?.[0]?.message;
      const rawContent = message?.content ?? "";
      const reasoningContent = message?.reasoning_content ?? "";
      const parser = new ToolifyParser(delimiter, thinkingEnabled);

      // ä¼˜å…ˆå¤„ç†åŸç”Ÿæ¨ç†å†…å®¹
      if (reasoningContent) {
        parser.feedReasoning(reasoningContent);
      }

      for (const char of rawContent) {
        parser.feedChar(char);
      }

      parser.finish();
      const events = parser.consumeEvents();

      const contentBlocks: any[] = [];
      let outputBuffer = "";
      for (const event of events) {
        if (event.type === "text") {
          contentBlocks.push({ type: "text", text: event.content });
          outputBuffer += event.content;
        } else if (event.type === "thinking") {
          // åœ¨éæµå¼å“åº”ä¸­ä¹Ÿè¿”å›æ€è€ƒå—,ä¿æŒä¸ Anthropic API æ ‡å‡†ä¸€è‡´
          contentBlocks.push({ type: "thinking", thinking: event.content });
          outputBuffer += event.content;
        } else if (event.type === "tool_call") {
          contentBlocks.push({
            type: "tool_use",
            id: `toolu_${crypto.randomUUID().split("-")[0]}`,
            name: event.call.name,
            input: event.call.arguments,
          });
          outputBuffer += JSON.stringify(event.call.arguments);
        }
      }

      // ç²¾ç¡®é‡æ–°è®¡ç®—è¾“å‡º Token
      const outputTokens = countTokensWithTiktoken(outputBuffer, request.model);

      return {
        id: `chatcmpl-${requestId}`,
        type: "message",
        role: "assistant",
        model: request.model, // ç¡®ä¿è¿”å›åŸå§‹æ¨¡å‹å
        content: contentBlocks.length > 0 ? contentBlocks : [{ type: "text", text: "" }],
        stop_reason: contentBlocks.some((b) => b.type === "tool_use")
          ? "tool_use"
          : (json.choices?.[0]?.finish_reason === "stop" ? "end_turn" : json.choices?.[0]?.finish_reason),
        stop_sequence: null,
        usage: {
          input_tokens: json.usage?.prompt_tokens ?? inputTokens,
          output_tokens: outputTokens || (json.usage?.completion_tokens ?? 0),
        },
      };
    } else {
      // Anthropic éæµå¼
      // ç¡®ä¿è¿”å›çš„æ¨¡å‹åæ˜¯åŸå§‹è¯·æ±‚çš„
      if (json && typeof json === 'object') {
        json.model = request.model;
      }
      return json;
    }
  }
}
